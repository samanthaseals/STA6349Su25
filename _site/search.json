[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Tentative Schedule",
    "section": "",
    "text": "Week 1: 06/17 & 06/19\n\n\n\n\n\n\nTuesday, 06/17/25\n\nLecture:\n\nProbability theory\n\nDeliverables:\n\nProbability theory (due M 06/23)\nGet to know you (due M 06/23)\n\n\n\n\nThursday, 06/19/25\n\nNo class: Juneteenth Holiday\n\n\n\n\n\n\n\n\n\n\n\nWeek 2: 06/24 & 06/26\n\n\n\n\n\n\nTuesday, 06/24\n\nLecture:\n\nRandom Variables and Their Distributions\n\n\n\n\nThursday, 06/28\n\nDeliverables:\n\nAssignment 1: Random Variables (due M 06/31)\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 3: 07/01 & 07/03\n\n\n\n\n\n\nTuesday, 07/01\n\nLecture:\n\nThinking Like a Bayesian\nThe Beta-Binomial Model\n\n\n\n\nThursday, 07/03\n\nLecture:\n\nConjugate families\nBalance and sequentiality\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 4: 07/08 & 07/10\n\n\n\n\n\n\nTuesday, 07/08\n\nDeliverables:\n\nAssignment 2: Conjugate families (due M 07/07)\n\n\n\n\nThursday, 07/10\n\nLecture:\n\nApproximating the posterior\nPosterior inference and prediction\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 5: 07/15 & 07/17\n\n\n\n\n\n\nTuesday, 07/15\n\nProject 1, in class work/planning\nDeliverables:\n\nRecording of project 1 presentation (due M 07/21)\nEvaluation of other groups (due M 07/19)\n\n\n\n\nThursday, 07/17\n\nLecture:\n\nSimple normal regression\nEvaluating regression models\nCategorical predictors\nMultiple predictors\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 6: 07/22 & 07/24\n\n\n\n\n\n\nTuesday, 07/22\n\nLecture:\n\nPoisson and negative binomial regressions\nLogistic regression\n\n\n\n\nThursday, 07/24\n\nDeliverables:\n\nAssignment: regression (due M 07/28)\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 7: 07/29 & 07/31\n\n\n\n\n\n\nTuesday, 07/29\n\nProject 2, in class work/planning\n\nYou will present in class on Thursday.\n\n\n\n\nThursday, 07/31\n\nIn class presentations!\n\nGroup 1\nGroup 2\nGroup 3\nGroup 4\nGroup 5\n\nDeliverables:\n\nEvaluation of other presentations (due M 08/04)\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 8: 08/05 & 08/07\n\n\n\n\n\n\nTuesday, 08/05\n\nDr.¬†Seals is out of town at JSM 2025.\nFinal Exam on Canvas\n\nEach student has 3 hours to complete the exam.\nOnce you open the exam, the timer will continue running.\nThe exam is open note/resources.\n\n\n\n\nThursday, 08/07\n\nDr.¬†Seals is on her way back from JSM 2025.\nCatch up\n\nPlease use this class time to catch up with anything you need to catch up with."
  },
  {
    "objectID": "files/slides/test.html#test",
    "href": "files/slides/test.html#test",
    "title": "Untitled",
    "section": "Test",
    "text": "Test\ntest\n\\[y = mx+b\\]"
  },
  {
    "objectID": "files/slides/intro-probability.html#introduction",
    "href": "files/slides/intro-probability.html#introduction",
    "title": "Introduction to Probability",
    "section": "Introduction",
    "text": "Introduction\n\nThe first few lectures come from Mathematical Statistics with Applications, by Wackerly.\n\nWe must understand the underlying probability and random variable theory before moving into the Bayesian world.\n\nWe will be covering the following chapters:\n\nChapter 2: probability theory\nChapter 3: discrete random variables\nChapter 4: continuous random variables"
  },
  {
    "objectID": "files/slides/intro-probability.html#probability-and-inference",
    "href": "files/slides/intro-probability.html#probability-and-inference",
    "title": "Introduction to Probability",
    "section": "2.2: Probability and Inference",
    "text": "2.2: Probability and Inference\n\nIn statistics we use probabilities to make inference, or draw conclusions.\nConsider a gambler who wishes to make an inference concerning the balance, or fairness, of a die.\n\nIf the die were perfectly balanced, one-sixth of the measurements in this population would be 1s, one-sixth would be 2s, one-sixth would be 3s, etc."
  },
  {
    "objectID": "files/slides/intro-probability.html#probability-and-inference-1",
    "href": "files/slides/intro-probability.html#probability-and-inference-1",
    "title": "Introduction to Probability",
    "section": "2.2: Probability and Inference",
    "text": "2.2: Probability and Inference\n\nUsing the scientific method, the gambler proposes the hypothesis that the die is balanced, and he seeks observations from nature to contradict the theory, if false.\n\nA sample of ten tosses is selected from the population by rolling the die ten times.\nAll ten tosses result in 1s. üßê\nThe gambler looks upon this output and concludes that his hypothesis is not in agreement with nature and, thus, the die is not balanced.\n\nThe gambler rejected his hypothesis not because it is impossible to throw ten 1s in ten tosses of a balanced die, but because it is highly improbable."
  },
  {
    "objectID": "files/slides/intro-probability.html#review-of-set-notation",
    "href": "files/slides/intro-probability.html#review-of-set-notation",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nSuppose the elements a_1, a_2, and a_3 are in the set A. A = \\left\\{ a_1, a_2, a_3 \\right\\}\n\nNotation: capital letters \\to sets of points.\n\nWe can denote the set of all elements under consideration with S, the universal set."
  },
  {
    "objectID": "files/slides/intro-probability.html#review-of-set-notation-1",
    "href": "files/slides/intro-probability.html#review-of-set-notation-1",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nA \\subset B:\n\nFor any two sets A and B, we say that A is a subset of B if every point in A is also in B."
  },
  {
    "objectID": "files/slides/intro-probability.html#review-of-set-notation-2",
    "href": "files/slides/intro-probability.html#review-of-set-notation-2",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nA \\cup B:\n\nThe union of A and B is the set of all points in either A or B.\ni.e., the union has all points that are in at least one of the sets."
  },
  {
    "objectID": "files/slides/intro-probability.html#review-of-set-notation-3",
    "href": "files/slides/intro-probability.html#review-of-set-notation-3",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nA \\cap B:\n\nThe intersection of A and B is the set of all points in both A and B.\ni.e., the intersection is where the two sets overlap."
  },
  {
    "objectID": "files/slides/intro-probability.html#review-of-set-notation-4",
    "href": "files/slides/intro-probability.html#review-of-set-notation-4",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\n\\bar{A} or A^c:\n\nThe complement of A is the set of points that are in S, but not in A.\nA \\cup \\bar{A} = S."
  },
  {
    "objectID": "files/slides/intro-probability.html#review-of-set-notation-5",
    "href": "files/slides/intro-probability.html#review-of-set-notation-5",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nA \\cap B = \\emptyset:\n\nA and B, are said to be disjoint or mutually exclusive when they have no points in common.\nRelated: for any set A, we know that A and \\bar{A} are mutually exclusive."
  },
  {
    "objectID": "files/slides/intro-probability.html#review-of-set-notation-6",
    "href": "files/slides/intro-probability.html#review-of-set-notation-6",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nFast forwarding through set algebra, we need to know these distributive laws:\n\n\n\\begin{align*}\nA \\cap (B \\cup C) &= (A \\cap B) \\cup (A \\cap C) \\\\\nA \\cup (B \\cap C) &= (A \\cup B) \\cap (A \\cup C) \\\\\n(A \\cap B)^c &= A^c \\cup B^c \\\\\n(A \\cup B)^c &= A^c \\cap B^c\n\\end{align*}"
  },
  {
    "objectID": "files/slides/intro-probability.html#review-of-set-notation-7",
    "href": "files/slides/intro-probability.html#review-of-set-notation-7",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nSuppose two dice are tossed and the numbers on the upper faces are observed.\nLet S denote the set of all possible pairs that can be observed.\n\ne.g., (2, 3) indicates that a 2 was observed on the first die and a 3 on the second.\n\nWhat are the observations in S?"
  },
  {
    "objectID": "files/slides/intro-probability.html#review-of-set-notation-8",
    "href": "files/slides/intro-probability.html#review-of-set-notation-8",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nDefine the following subsets of S and list their points:\n\nA: The number on the second die is even. \nB: The sum of the two numbers is even. \nC: At least one number in the pair is odd."
  },
  {
    "objectID": "files/slides/intro-probability.html#review-of-set-notation-9",
    "href": "files/slides/intro-probability.html#review-of-set-notation-9",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nList the points in the following:\n\nA \\cup B \nA \\cap B \nA \\cap C \nB \\cup C"
  },
  {
    "objectID": "files/slides/intro-probability.html#review-of-set-notation-10",
    "href": "files/slides/intro-probability.html#review-of-set-notation-10",
    "title": "Introduction to Probability",
    "section": "2.3: Review of Set Notation",
    "text": "2.3: Review of Set Notation\n\nList the points in the following:\n\nA \\cup B^c\nA^c \\cap B\nA \\cap C^c\nB \\cup C^c"
  },
  {
    "objectID": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment",
    "href": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment",
    "title": "Introduction to Probability",
    "section": "2.4: A Probabilistic Model for an Experiment",
    "text": "2.4: A Probabilistic Model for an Experiment\n\nExperiment: the process by which an observation is made.\n\nExamples:\n\ncoin and die tossing,\nmeasuring the systolic blood pressure of an individual,\ndetermine the number of bacteria per cubic centimeter in a serving of processed food."
  },
  {
    "objectID": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment-1",
    "href": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment-1",
    "title": "Introduction to Probability",
    "section": "2.4: A Probabilistic Model for an Experiment",
    "text": "2.4: A Probabilistic Model for an Experiment\n\nEvents: the outcomes possible in an experiment.\n\nNotation: capital leters\nExamples for bacteria observation:\n\nA: Exactly 110 bacteria are present.\nB: More than 200 bacteria are present.\nC: The number of bacteria present is between 100 and 300."
  },
  {
    "objectID": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment-2",
    "href": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment-2",
    "title": "Introduction to Probability",
    "section": "2.4: A Probabilistic Model for an Experiment",
    "text": "2.4: A Probabilistic Model for an Experiment\n\nSample space, S: the set consisting of all possible sample points.\nThe following Venn diagram shows an example of six simple events in S,"
  },
  {
    "objectID": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment-3",
    "href": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment-3",
    "title": "Introduction to Probability",
    "section": "2.4: A Probabilistic Model for an Experiment",
    "text": "2.4: A Probabilistic Model for an Experiment\n\nCompound event: A collection of sample points in a discrete sample space, S.\ne.g., suppose we define two events, A and B,"
  },
  {
    "objectID": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment-4",
    "href": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment-4",
    "title": "Introduction to Probability",
    "section": "2.4: A Probabilistic Model for an Experiment",
    "text": "2.4: A Probabilistic Model for an Experiment\n\nSuppose S is a sample space associated with an experiment. To every event A in S (i.e., A \\subset S), we assign a number, P[A], called the probability of A, such that the follow axioms hold:\n\nAxiom 1: P[A] \\ge 0.\nAxiom 2: P[S] = 1.\nAxiom 3: If A_1, A_2, ..., A_n form a sequence of pairwise mutually exclusive (A_i \\cap A_j = \\emptyset if i \\ne j) events in S, then P[A_1 \\cup A_2 \\cup \\ ... \\cup \\ A_n] = \\sum_{i=1}^n P[A_i]"
  },
  {
    "objectID": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment-5",
    "href": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment-5",
    "title": "Introduction to Probability",
    "section": "2.4: A Probabilistic Model for an Experiment",
    "text": "2.4: A Probabilistic Model for an Experiment\n\nSuppose a sample space consists of five simple events, E_1, E_2, E_3, E_4, and E_5.\n\nIf P[E_1] = P[E_2] = 0.15, P[E_3] = 0.4, and P[E_4] = 2P[E_5], find the probabilities of E_4 and E_5."
  },
  {
    "objectID": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment-6",
    "href": "files/slides/intro-probability.html#a-probabilistic-model-for-an-experiment-6",
    "title": "Introduction to Probability",
    "section": "2.4: A Probabilistic Model for an Experiment",
    "text": "2.4: A Probabilistic Model for an Experiment\n\nSuppose a sample space consists of five simple events, E_1, E_2, E_3, E_4, and E_5.\n\nIf P[E_1] = 3P[E_2] = 0.3, find the probabilities of the remaining simple events if you know that the remaining simple events are equally probable."
  },
  {
    "objectID": "files/slides/intro-probability.html#calculating-the-probability-of-an-event",
    "href": "files/slides/intro-probability.html#calculating-the-probability-of-an-event",
    "title": "Introduction to Probability",
    "section": "2.5: Calculating the Probability of an Event",
    "text": "2.5: Calculating the Probability of an Event\n\nThe following are steps used to find the probability of an event:\n\nDefine the experiment and clearly determine how to describe one simple event.\nDefine S: list the simple events associated with the experiment.\nAssign reasonable probabilities to the sample points in S; remember that P[E_i] \\ge 0 and \\sum_i P[E_i] = 1.\nDefine the event of interest, A, as a specific collection of sample points.\nFind P[A] by summing the probabilities of the sample points in A."
  },
  {
    "objectID": "files/slides/intro-probability.html#calculating-the-probability-of-an-event-1",
    "href": "files/slides/intro-probability.html#calculating-the-probability-of-an-event-1",
    "title": "Introduction to Probability",
    "section": "2.5: Calculating the Probability of an Event",
    "text": "2.5: Calculating the Probability of an Event\n\nSuppose we toss a balanced coin three times. Find the probability that 2/3 tosses result in heads.\n\nDefine the experiment and clearly determine how to describe one simple event.\nDefine S: list the simple events associated with the experiment."
  },
  {
    "objectID": "files/slides/intro-probability.html#calculating-the-probability-of-an-event-2",
    "href": "files/slides/intro-probability.html#calculating-the-probability-of-an-event-2",
    "title": "Introduction to Probability",
    "section": "2.5: Calculating the Probability of an Event",
    "text": "2.5: Calculating the Probability of an Event\n\nSuppose we toss a balanced coin three times. Find the probability that 2/3 tosses result in heads.\n\nDefine S: list the simple events associated with the experiment.\nAssign reasonable probabilities to the sample points in S; remember that P[E_i] \\ge 0 and \\sum_i P[E_i] = 1."
  },
  {
    "objectID": "files/slides/intro-probability.html#calculating-the-probability-of-an-event-3",
    "href": "files/slides/intro-probability.html#calculating-the-probability-of-an-event-3",
    "title": "Introduction to Probability",
    "section": "2.5: Calculating the Probability of an Event",
    "text": "2.5: Calculating the Probability of an Event\n\nSuppose we toss a balanced coin three times. Find the probability that 2/3 tosses result in heads.\n\nDefine the event of interest, A, as a specific collection of sample points.\nFind P[A] by summing the probabilities of the sample points in A."
  },
  {
    "objectID": "files/slides/intro-probability.html#section",
    "href": "files/slides/intro-probability.html#section",
    "title": "Introduction to Probability",
    "section": "",
    "text": "Break time!"
  },
  {
    "objectID": "files/slides/intro-probability.html#tools-for-counting-sample-points",
    "href": "files/slides/intro-probability.html#tools-for-counting-sample-points",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nLet us discuss some results from combinatorial theory.\nWe want to be able to count the total number of sample points in the sample space, S.\nSometimes we use this method to efficiently find probabilities.\n\nIf a sample space contains N equiprobable sample points and an event A contains exactly n_a sample points, then\n\n\n\nP[A] = \\frac{n_a}{N}"
  },
  {
    "objectID": "files/slides/intro-probability.html#tools-for-counting-sample-points-1",
    "href": "files/slides/intro-probability.html#tools-for-counting-sample-points-1",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nTheorem:\n\nWith m elements a_1, a_2, ‚Ä¶, a_m and n elements b_1, b_2, ‚Ä¶, b_n, it is possible for form mn = m \\times n pairs containing one element from each group."
  },
  {
    "objectID": "files/slides/intro-probability.html#tools-for-counting-sample-points-2",
    "href": "files/slides/intro-probability.html#tools-for-counting-sample-points-2",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nConsider an experiment that consists of recording the birthday for each of 20 randomly selected persons.\nIgnoring leap years and assuming that there are only 365 possible distinct birthdays, find the number of points in the sample space S for this experiment."
  },
  {
    "objectID": "files/slides/intro-probability.html#tools-for-counting-sample-points-3",
    "href": "files/slides/intro-probability.html#tools-for-counting-sample-points-3",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nConsider an experiment that consists of recording the birthday for each of 20 randomly selected persons.\nIf we assume that each of the possible sets of birthdays is equiprobable, what is the probability that each person in the 20 has a different birthday?"
  },
  {
    "objectID": "files/slides/intro-probability.html#tools-for-counting-sample-points-4",
    "href": "files/slides/intro-probability.html#tools-for-counting-sample-points-4",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nPermutation: an ordered arrangement of r distinct objects, P_r^n.\n\n\nP_r^n = \\frac{n!}{(n-r)!}\n\n\nCombination: the number of subsets formed from n objects taken r at a time, C_r^n or {n \\choose r}.\n\n\nC_r^n = {n \\choose r} = \\frac{P_r^n}{r!} = \\frac{n!}{r(n-r)!}"
  },
  {
    "objectID": "files/slides/intro-probability.html#tools-for-counting-sample-points-5",
    "href": "files/slides/intro-probability.html#tools-for-counting-sample-points-5",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nRemember, factorials are defined as follows:\n\n\nn! = n \\times (n-1) \\times (n-2) \\times ... \\times 2 \\times 1\n\n\nWith special factorials:\n\n\n\\begin{align*}\n1! &= 1 \\\\\n0! &= 1\n\\end{align*}"
  },
  {
    "objectID": "files/slides/intro-probability.html#tools-for-counting-sample-points-6",
    "href": "files/slides/intro-probability.html#tools-for-counting-sample-points-6",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nSuppose that an assembly operation in a manufacturing plant involves four steps, which can be performed in any sequence. If the manufacturer wishes to compare the assembly time for each of the sequences, how many different sequences will be involved in the experiment?"
  },
  {
    "objectID": "files/slides/intro-probability.html#tools-for-counting-sample-points-7",
    "href": "files/slides/intro-probability.html#tools-for-counting-sample-points-7",
    "title": "Introduction to Probability",
    "section": "2.6: Tools for Counting Sample Points",
    "text": "2.6: Tools for Counting Sample Points\n\nA company orders supplies from M distributors and wishes to place n orders (n&lt;M). Assume that the company places the orders in a manner that allows every distributor an equal chance of obtaining any one order and there is no restriction on the number of orders that can be placed with any distributor. Find the probability that a particular distributor gets exactly k orders (k \\le n)."
  },
  {
    "objectID": "files/slides/intro-probability.html#conditional-prob.-and-independence-of-events",
    "href": "files/slides/intro-probability.html#conditional-prob.-and-independence-of-events",
    "title": "Introduction to Probability",
    "section": "2.7: Conditional Prob. and Independence of Events",
    "text": "2.7: Conditional Prob. and Independence of Events\n\nConditional probability of an event A given that an event B has occurred is as follows\n\n\nP[A|B] = \\frac{P[A \\cap B]}{P[B]},\n\n\nso long as P[B] &gt; 0.\nAlgebraically equivalent,\n\n\nP[A \\cap B] = P[A|B] \\times P[B] \\ \\ \\ \\ \\ \\& \\ \\ \\ \\ \\ P[B] = \\frac{P[A \\cap B]}{P[A|B]}"
  },
  {
    "objectID": "files/slides/intro-probability.html#conditional-prob.-and-independence-of-events-1",
    "href": "files/slides/intro-probability.html#conditional-prob.-and-independence-of-events-1",
    "title": "Introduction to Probability",
    "section": "2.7: Conditional Prob. and Independence of Events",
    "text": "2.7: Conditional Prob. and Independence of Events\n\nSuppose that a balanced die is tossed once. Find the probability of rolling a 1, given that an odd number was obtained."
  },
  {
    "objectID": "files/slides/intro-probability.html#conditional-prob.-and-independence-of-events-2",
    "href": "files/slides/intro-probability.html#conditional-prob.-and-independence-of-events-2",
    "title": "Introduction to Probability",
    "section": "2.7: Conditional Prob. and Independence of Events",
    "text": "2.7: Conditional Prob. and Independence of Events\n\nTwo events A and B are said to be independent events if any of the following holds:\n\n\n\\begin{align*}\nP[A|B] &= P[A] \\\\\nP[B|A] &= P[B] \\\\\nP[A \\cap B] &= P[A] P[B]\n\\end{align*}\n\n\nOtherwise, we say that A and B are dependent events."
  },
  {
    "objectID": "files/slides/intro-probability.html#conditional-prob.-and-independence-of-events-3",
    "href": "files/slides/intro-probability.html#conditional-prob.-and-independence-of-events-3",
    "title": "Introduction to Probability",
    "section": "2.7: Conditional Prob. and Independence of Events",
    "text": "2.7: Conditional Prob. and Independence of Events\n\nConsider the following events in the toss of a single die:\n\nA: Observe an odd number.\nB: Observe an even number.\nC: Observe a 1 or 2.\n\nAre A and B independent events? \nAre A and C independent events?"
  },
  {
    "objectID": "files/slides/intro-probability.html#two-laws-of-probability",
    "href": "files/slides/intro-probability.html#two-laws-of-probability",
    "title": "Introduction to Probability",
    "section": "2.8: Two Laws of Probability",
    "text": "2.8: Two Laws of Probability\n\nTheorem: The Multiplicative Law of Probability\n\nThe probability of the intersection of two events A and B is\n\n\n\\begin{align*}P[A\\cap B] &= P[A] P[B|A] \\\\ &= P[B] P[A|B]\\end{align*}\n\nNote that if A and B are independent, then\n\nP[A \\cap B] = P[A] P[B]"
  },
  {
    "objectID": "files/slides/intro-probability.html#two-laws-of-probability-1",
    "href": "files/slides/intro-probability.html#two-laws-of-probability-1",
    "title": "Introduction to Probability",
    "section": "2.8: Two Laws of Probability",
    "text": "2.8: Two Laws of Probability\n\nTheorem: The Additive Law of Probability\n\nThe probability of the union of two events A and B is\n\n\nP[A \\cup B] = P[A] + P[B] - P[A \\cap B] \n\nNote that if A and B are mutually exclusive, then P[A \\cap B] = 0 and\n\nP[A \\cup B] = P[A] + P[B]"
  },
  {
    "objectID": "files/slides/intro-probability.html#two-laws-of-probability-2",
    "href": "files/slides/intro-probability.html#two-laws-of-probability-2",
    "title": "Introduction to Probability",
    "section": "2.8: Two Laws of Probability",
    "text": "2.8: Two Laws of Probability\n\nTheorem: The Complement Rule\n\nIf A is an event, then\n\n\n\n\\begin{align*}\nP[A] + P[A^c] &= 1 \\\\\nP[A] &= 1 - P[A^c] \\\\\nP[A^c] &= 1 - P[A] \\\\\n\\end{align*}"
  },
  {
    "objectID": "files/slides/intro-probability.html#calculating-the-probability-of-an-event-4",
    "href": "files/slides/intro-probability.html#calculating-the-probability-of-an-event-4",
    "title": "Introduction to Probability",
    "section": "2.9: Calculating the Probability of an Event",
    "text": "2.9: Calculating the Probability of an Event\nThe steps used to define the probability of an event:\n\nDefine the experiment.\nVisualize the nature of the sample points. Identify a few to clarify your thinking.\nWrite an equation expressing the event of interest (A) as a composition of two or more events, using usions, intersections, and/or complements. Make certain that event A and the event implied by the compsotion represnt the sameset of sample points.\nApply the additive and multiplicative laws of probability in the compositions obtained in step 3 to find P[A]."
  },
  {
    "objectID": "files/slides/intro-probability.html#calculating-the-probability-of-an-event-5",
    "href": "files/slides/intro-probability.html#calculating-the-probability-of-an-event-5",
    "title": "Introduction to Probability",
    "section": "2.9: Calculating the Probability of an Event",
    "text": "2.9: Calculating the Probability of an Event\n\nIt is known that a patient with a disease with respond to treatment with probability equal to 0.9. If three patients with the disease are treated independently, find the probability that at least one will respond."
  },
  {
    "objectID": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule",
    "href": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes‚Äô Rule",
    "text": "2.10: The Law of Total Probability and Bayes‚Äô Rule\n\nPartition:\n\nFor some positive integer k, let the sets B_1, B_2, ..., B_k be such that\n\nS = B_1 \\cup B_2 \\cup ... \\cup B_k\nB_1 \\cap B_j = \\emptyset, for i \\ne j\n\nThen the collection of sets \\{B_1, B_2, ..., B_k\\} is said to be a partition of S."
  },
  {
    "objectID": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-1",
    "href": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-1",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes‚Äô Rule",
    "text": "2.10: The Law of Total Probability and Bayes‚Äô Rule\n\nWe also know that if A is any subset of S and \\{B_1, B_2, ..., B_k\\} is a partition of S, A can be decomposed:\n\n\nA = (A \\cap B_1) \\cup (A \\cap B_2) \\cup \\ ... \\ \\cup (A \\cap B_k)"
  },
  {
    "objectID": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-2",
    "href": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-2",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes‚Äô Rule",
    "text": "2.10: The Law of Total Probability and Bayes‚Äô Rule\n\nTheorem:\n\nAssume that \\{ B_1, B_2, ..., B_k \\} is a partition of S such that P[B_i] &gt; 0 for i = 1, 2, ..., k. Then for any event A,\n\n\n\nP[A] = \\sum_{i=1}^k P[A|B_i] P[B_i]"
  },
  {
    "objectID": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-3",
    "href": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-3",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes‚Äô Rule",
    "text": "2.10: The Law of Total Probability and Bayes‚Äô Rule\n\nTheorem: Bayes‚Äô Rule\n\nAssume that \\{ B_1, B_2, ..., B_k \\} is a partition of S such that P[B_i] &gt; 0 for i = 1, 2, ..., k. Then\n\n\n\n\\begin{align*}\nP[B_j | A] &= \\frac{P[B_j \\cap A]}{P[A]} \\\\\n&= \\frac{P[B_j \\cap A]}{P[A \\cap B_1] + ... + P[A \\cap B_k]} \\\\\n&= \\frac{P[B_j] \\times P[A|B_j]}{P[B_1]\\times P[A|B_1] + ... + P[B_k] \\times P[A|B_k]} \\\\\n&= \\frac{P[B_j] \\times P[A|B_j] }{\\sum_{i=1}^k P[A|B_i] \\times P[B_i]}\n\\end{align*}"
  },
  {
    "objectID": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-4",
    "href": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-4",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes‚Äô Rule",
    "text": "2.10: The Law of Total Probability and Bayes‚Äô Rule\n\nOf the travelers arriving at a small airport, 60% fly on major airlines, 30% fly on privately owned planes, and the remainder fly on commercially owned planes not belonging to a major airline. Of those traveling on major airlines, 50% are traveling for business reasons, whereas 60% of those arriving on private planes and 90% of those arriving on other commercially owned planes are traveling for business reasons.\nSuppose that we randomly select one person arriving at this airport. What is the probability that the person:\n\nis traveling on business?"
  },
  {
    "objectID": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-5",
    "href": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-5",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes‚Äô Rule",
    "text": "2.10: The Law of Total Probability and Bayes‚Äô Rule\n\nOf the travelers arriving at a small airport, 60% fly on major airlines, 30% fly on privately owned planes, and the remainder fly on commercially owned planes not belonging to a major airline. Of those traveling on major airlines, 50% are traveling for business reasons, whereas 60% of those arriving on private planes and 90% of those arriving on other commercially owned planes are traveling for business reasons.\nSuppose that we randomly select one person arriving at this airport. What is the probability that the person:\n\nis traveling for business on a privately owned plane?"
  },
  {
    "objectID": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-6",
    "href": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-6",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes‚Äô Rule",
    "text": "2.10: The Law of Total Probability and Bayes‚Äô Rule\n\nOf the travelers arriving at a small airport, 60% fly on major airlines, 30% fly on privately owned planes, and the remainder fly on commercially owned planes not belonging to a major airline. Of those traveling on major airlines, 50% are traveling for business reasons, whereas 60% of those arriving on private planes and 90% of those arriving on other commercially owned planes are traveling for business reasons.\nSuppose that we randomly select one person arriving at this airport. What is the probability that the person:\n\narrived on a privately owned plane, given that the person is traveling for business reasons?"
  },
  {
    "objectID": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-7",
    "href": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-7",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes‚Äô Rule",
    "text": "2.10: The Law of Total Probability and Bayes‚Äô Rule\n\nOf the travelers arriving at a small airport, 60% fly on major airlines, 30% fly on privately owned planes, and the remainder fly on commercially owned planes not belonging to a major airline. Of those traveling on major airlines, 50% are traveling for business reasons, whereas 60% of those arriving on private planes and 90% of those arriving on other commercially owned planes are traveling for business reasons.\nSuppose that we randomly select one person arriving at this airport. What is the probability that the person:\n\nis traveling on business, given that the person is flying on a commercially owned plane?"
  },
  {
    "objectID": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-8",
    "href": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-8",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes‚Äô Rule",
    "text": "2.10: The Law of Total Probability and Bayes‚Äô Rule\n\nLet D be the event that a person has a rare disease with an incidence rate of 1% in the population. i.e., P[D]=0.01. Suppose a machine is used to diagnose the disease. Let C be the eventt aht the disease is confirmed as the diagnosis. Suppose that the probaiblity of the machine falsely confirming the disease when one doesn‚Äôt have it (aka, a false positive) is P[C|D^c] = 0.15. Further, the probability that the machine correctly confirms the disease is P[C|D]=0.95.\nNow, suppose that the machine confirms that a person has the disease. What is the probability that the person actually has the disease? i.e., find P[D|C]."
  },
  {
    "objectID": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-9",
    "href": "files/slides/intro-probability.html#the-law-of-total-probability-and-bayes-rule-9",
    "title": "Introduction to Probability",
    "section": "2.10: The Law of Total Probability and Bayes‚Äô Rule",
    "text": "2.10: The Law of Total Probability and Bayes‚Äô Rule\n\nIn a production line, 8% of all items produced are defective, 75% of all defective items are fully inspected, while 10% of all non-defective items go through a complete inspection. Given that an item is completely inspected, what is the probability that it is defective?"
  },
  {
    "objectID": "files/slides/intro-probability.html#numerical-events-and-random-variables",
    "href": "files/slides/intro-probability.html#numerical-events-and-random-variables",
    "title": "Introduction to Probability",
    "section": "2.11: Numerical Events and Random Variables",
    "text": "2.11: Numerical Events and Random Variables\n\nRandom variable:\n\nA real-valued function for which the domain is a sample space.\n\nLet Y denote a variable to be measured in an experiment.\n\nBecause the value of Y will vary depending on the outcome of the experiment, it is called a random variable.\nEach point in the sample space will be assigned a real number denoting the value of Y.\n\nThat is, it may vary from one sample point to another."
  },
  {
    "objectID": "files/slides/intro-probability.html#numerical-events-and-random-variables-1",
    "href": "files/slides/intro-probability.html#numerical-events-and-random-variables-1",
    "title": "Introduction to Probability",
    "section": "2.11: Numerical Events and Random Variables",
    "text": "2.11: Numerical Events and Random Variables\n\nDefine an experiment as tossing two coins and observing the results. Let Y equal the number of heads obtained.\n\nIdentify the sample points in S. \nAssign a value to each sample point"
  },
  {
    "objectID": "files/slides/intro-probability.html#numerical-events-and-random-variables-2",
    "href": "files/slides/intro-probability.html#numerical-events-and-random-variables-2",
    "title": "Introduction to Probability",
    "section": "2.11: Numerical Events and Random Variables",
    "text": "2.11: Numerical Events and Random Variables\n\nDefine an experiment as tossing two coins and observing the results. Let Y equal the number of heads obtained.\n\nIdentify the sample points associated with each value of the random variable Y. \nCompute the probabilities for each value of Y."
  },
  {
    "objectID": "files/slides/intro-probability.html#homework",
    "href": "files/slides/intro-probability.html#homework",
    "title": "Introduction to Probability",
    "section": "Homework",
    "text": "Homework\n\n\n\n2.15\n\n2.18\n\n2.32\n\n2.33\n\n2.51\n\n2.54\n\n2.73\n\n2.77\n\n\n\n2.94\n\n2.106\n\n2.107\n\n2.114\n\n2.120\n\n2.128\n\n2.140\n\n2.141"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA6349 - Applied Bayesian Analysis - Summer 2025",
    "section": "",
    "text": "Welcome to Applied Bayesian Analysis for Summer 2025!\nEach class meeting will be split between lecture and practice. Attending live is not necessary (with the exception of 07/31), but it is your responsibility to keep up with the assigned work.\nThe final exam will be available on Canvas on Tuesday, August 5, 2025.\nThis project was supported by the 2023 BATS program, funded by NSF IUSE: EHR program with award numbers 2215879, 2215920, and 2215709."
  },
  {
    "objectID": "files/slides/intro-rv.html#introduction",
    "href": "files/slides/intro-rv.html#introduction",
    "title": "Random Variables and their Distributions",
    "section": "Introduction",
    "text": "Introduction\n\nThe first few lectures come from Mathematical Statistics with Applications, by Wackerly.\n\nWe must understand the underlying probability and random variable theory before moving into the Bayesian world.\n\nWe will be covering the following chapters:\n\nChapter 2: probability theory\nChapter 3: discrete random variables\nChapter 4: continuous random variables"
  },
  {
    "objectID": "files/slides/intro-rv.html#basic-definitions",
    "href": "files/slides/intro-rv.html#basic-definitions",
    "title": "Random Variables and their Distributions",
    "section": "3.1: Basic Definitions",
    "text": "3.1: Basic Definitions\n\nDiscrete random variable: a variable that can assume only a finite or countably infinite number of distinct values.\nProbability distribution of a random variable: collection of probabilities for each value of the random variable.\nNotation:\n\nUppercase letter (e.g., Y) denotes a random variable.\nLowercase letter (e.g., y) denotes a particular value that the random variable may assume.\n\nThe specific observed value, y, is not random."
  },
  {
    "objectID": "files/slides/intro-rv.html#probability-distributions-for-discrete-rv",
    "href": "files/slides/intro-rv.html#probability-distributions-for-discrete-rv",
    "title": "Random Variables and their Distributions",
    "section": "3.2: Probability Distributions for Discrete RV",
    "text": "3.2: Probability Distributions for Discrete RV\n\nProbability function for \\boldsymbol Y: sum of the the probabilities of all sample points in S that are assigned the value y\n\nP[Y = y] = p(y): the probability that Y takes on the value y.\n\nProbability distribution for \\boldsymbol Y: a formula, table, or graph that provides p(y) \\ \\forall \\ y.\nTheorem:\n\nFor any discrete probability distribution, the following must be true:\n\n0 \\le p(y) \\le 1 \\  \\forall \\ y\n\\sum_y p(y) = 1 \\ \\forall \\ p(y) &gt; 0."
  },
  {
    "objectID": "files/slides/intro-rv.html#expected-values",
    "href": "files/slides/intro-rv.html#expected-values",
    "title": "Random Variables and their Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nExpected value: Let Y be a discrete random variable with probability function p(y). Then the expected value of Y, E[Y], is defined to be\n\n\nE(Y) = \\sum_{y} y p(y)\n\n\nWhen p(y) is an accurate characterization of the population frequency distribution, then the expected value is the population mean.\n\n\nE[Y] = \\mu"
  },
  {
    "objectID": "files/slides/intro-rv.html#expected-values-1",
    "href": "files/slides/intro-rv.html#expected-values-1",
    "title": "Random Variables and their Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and g(Y) be a real-valued function of Y (i.e., a transformed variable). Then the expected value of g(Y) is given by\n\n\n\nE[g(Y)] = \\sum_{y} g(y) p(y)"
  },
  {
    "objectID": "files/slides/intro-rv.html#expected-values-2",
    "href": "files/slides/intro-rv.html#expected-values-2",
    "title": "Random Variables and their Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nVariance: if Y is a random variable with mean E[Y] = \\mu, the variance of a random variable Y is defined to be the expected value of (Y-\\mu)^2.\n\n\nV[Y] = E\\left[ (Y-\\mu)^2 \\right]\n\n\nIf p(y) is an accurate characterization of the population frequency distribution, then V(Y) is the population variance,\n\n\nV[Y] = \\sigma^2\n\n\nStandard deviation: the positive square root of V[Y]."
  },
  {
    "objectID": "files/slides/intro-rv.html#expected-values-3",
    "href": "files/slides/intro-rv.html#expected-values-3",
    "title": "Random Variables and their Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nThe probability distribution for a random variable Y is given below.\n\n\n\nFind the mean, variance, and standard deviation of Y."
  },
  {
    "objectID": "files/slides/intro-rv.html#expected-values-4",
    "href": "files/slides/intro-rv.html#expected-values-4",
    "title": "Random Variables and their Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and c be a constant. Then,\n\n\nE(c) = c\n\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y), g(Y) be a function of Y, and c be a constant. Then,\n\n\nE[cg(Y)] = cE[g(Y)]\n\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y), and g_1(Y), g_2(Y), ..., g_k(Y) be k functions of Y. Then,\n\n\nE[g_1(Y) + g_2(Y) + ... + g_k(Y)] = E[g_1(Y)] + E[g_2(Y)] + ... + E[g_k(Y)]"
  },
  {
    "objectID": "files/slides/intro-rv.html#expected-values-5",
    "href": "files/slides/intro-rv.html#expected-values-5",
    "title": "Random Variables and their Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nPutting the previous theorems into one:\nTheorem:\n\nLet Y be a discrete random variable with probability function p(y) and mean E[Y] = \\mu. Then,\n\n\nV[Y] = \\sigma^2 = E\\left[(Y-\\mu)^2\\right] = E\\left[Y^2\\right] - \\mu^2"
  },
  {
    "objectID": "files/slides/intro-rv.html#expected-values-6",
    "href": "files/slides/intro-rv.html#expected-values-6",
    "title": "Random Variables and their Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nThe probability distribution for a random variable Y is given below.\n\n\n\nUse the previous theorem to find V[Y] and compare to our previous answer.\n\nRecall that \\mu=1.75."
  },
  {
    "objectID": "files/slides/intro-rv.html#expected-values-7",
    "href": "files/slides/intro-rv.html#expected-values-7",
    "title": "Random Variables and their Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nLet Y be a random variable with p(y) in the table below.\n\n\n\nFind\n\nE[Y] \nE[1/Y] \nE\\left[Y^2-1\\right] \nV[Y]"
  },
  {
    "objectID": "files/slides/intro-rv.html#expected-values-8",
    "href": "files/slides/intro-rv.html#expected-values-8",
    "title": "Random Variables and their Distributions",
    "section": "3.3: Expected Values",
    "text": "3.3: Expected Values\n\nE[Y] \nE[1/Y] \nE\\left[Y^2-1\\right] \nV[Y]"
  },
  {
    "objectID": "files/slides/intro-rv.html#binomial-probability-distribution",
    "href": "files/slides/intro-rv.html#binomial-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\n\nBinomial experiment:\n\nThe experiment consists of a fixed number, n, of identical trials.\nEach trial results in one of two outcomes: success (S) or failure (F).\nThe probability of success on a single trial is equal to some value p and remains the same from trial to trial.\n\nThe probability of failure is equal to q = (1-p).\n\nThe trials are independent.\nThe random variable of interest is Y, the number of successes observed during the n trials."
  },
  {
    "objectID": "files/slides/intro-rv.html#binomial-probability-distribution-1",
    "href": "files/slides/intro-rv.html#binomial-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\n\nBinomial Distribution\n\nA random variable Y is said to have a binomial distribution based on n trials with success probability p iff\n\n\n\np(y) = {n \\choose y}p^y q^{n-y}, \\text{ where } y = 0, 1, 2, ..., n, \\text{ and } 0 \\le p \\le1\n\n\nTheorem:\n\nLet Y be a binomial random variable based on n trials and success probability p. Then\n\n\n\nE[Y] = \\mu = np \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2 = npq\n\n\nSee Wackerly pg. 107 for derivation."
  },
  {
    "objectID": "files/slides/intro-rv.html#binomial-probability-distribution-2",
    "href": "files/slides/intro-rv.html#binomial-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\np=0.10"
  },
  {
    "objectID": "files/slides/intro-rv.html#binomial-probability-distribution-3",
    "href": "files/slides/intro-rv.html#binomial-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\np=0.25"
  },
  {
    "objectID": "files/slides/intro-rv.html#binomial-probability-distribution-4",
    "href": "files/slides/intro-rv.html#binomial-probability-distribution-4",
    "title": "Random Variables and their Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\np=0.50"
  },
  {
    "objectID": "files/slides/intro-rv.html#binomial-probability-distribution-5",
    "href": "files/slides/intro-rv.html#binomial-probability-distribution-5",
    "title": "Random Variables and their Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\np=0.75"
  },
  {
    "objectID": "files/slides/intro-rv.html#binomial-probability-distribution-6",
    "href": "files/slides/intro-rv.html#binomial-probability-distribution-6",
    "title": "Random Variables and their Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\np=0.90"
  },
  {
    "objectID": "files/slides/intro-rv.html#binomial-probability-distribution-7",
    "href": "files/slides/intro-rv.html#binomial-probability-distribution-7",
    "title": "Random Variables and their Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\n\nWhat do you notice when comparing distributions under p vs.¬†1-p?"
  },
  {
    "objectID": "files/slides/intro-rv.html#binomial-probability-distribution-8",
    "href": "files/slides/intro-rv.html#binomial-probability-distribution-8",
    "title": "Random Variables and their Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\n\nWhat do you notice as n increases?"
  },
  {
    "objectID": "files/slides/intro-rv.html#binomial-probability-distribution-9",
    "href": "files/slides/intro-rv.html#binomial-probability-distribution-9",
    "title": "Random Variables and their Distributions",
    "section": "3.4: Binomial Probability Distribution",
    "text": "3.4: Binomial Probability Distribution\n\nThe manufacturer of a low-calorie dairy drink wishes to compare the taste appeal of a new formula (formula B) with that of the standard formula (formula A). Each of four judges is given three glasses in random order, two containing formula A and the other containing formula B. Each judge is asked to state which glass he or she most enjoyed. Suppose that the two formulas are equally attractive. Let Y be the number of judges stating a preference for the new formula.\n\nFind the probability function for Y. \nWhat is the probability that at least three of the four judges state a preference for the new formula?\nFind the expected value of Y.\nFind the variance of Y."
  },
  {
    "objectID": "files/slides/intro-rv.html#poisson-probability-distribution",
    "href": "files/slides/intro-rv.html#poisson-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "3.8: Poisson Probability Distribution",
    "text": "3.8: Poisson Probability Distribution\n\nThe Poisson probability distribution often provides a good model for the probability distribution of the number Y of rare events that occur in space, time, volume, or any other dimension.\nPoisson Distribution:\n\nA random variable Y is said to have a Poisson probability distribution iff\n\n\n\np(y) = \\frac{\\lambda^y}{y!}e^{-\\lambda}, \\text{ where } y=0,1,2,..., \\text{ and } \\lambda &gt; 0\n\n\nTheorem\n\nIf Y is a random variable with a Poisson distribution with parameter \\lambda, then\n\n\n\nE[Y] = \\mu = \\lambda \\text{ and } V[Y] = \\sigma^2 = \\lambda\n\n\nSee Wackerly pg. 134 for derivation."
  },
  {
    "objectID": "files/slides/intro-rv.html#poisson-probability-distribution-1",
    "href": "files/slides/intro-rv.html#poisson-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "3.8: Poisson Probability Distribution",
    "text": "3.8: Poisson Probability Distribution\n\nCustomers arrive at a checkout counter in a department store according to a Poisson distribution at an average of seven per hour. During a given hour, what are the probabilities that\n\nno more than three customers arrive? \nat least two customers arrive? \nexactly five customers arrive?"
  },
  {
    "objectID": "files/slides/intro-rv.html#homework",
    "href": "files/slides/intro-rv.html#homework",
    "title": "Random Variables and their Distributions",
    "section": "Homework",
    "text": "Homework\n\n3.6\n3.10\n3.15\n3.22\n3.34\n3.60\n3.128\n3.129\n3.136"
  },
  {
    "objectID": "files/slides/intro-rv.html#introduction-1",
    "href": "files/slides/intro-rv.html#introduction-1",
    "title": "Random Variables and their Distributions",
    "section": "Introduction",
    "text": "Introduction\n\nThe first few lectures come from Mathematical Statistics with Applications, by Wackerly.\n\nWe must understand the underlying probability and random variable theory before moving into the Bayesian world.\n\nWe will be covering the following chapters:\n\nChapter 2: probability theory\nChapter 3: discrete random variables\nChapter 4: continuous random variables"
  },
  {
    "objectID": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv",
    "href": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv",
    "title": "Random Variables and their Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nDistribution function:\n\nLet Y denote any random variable. The distribution function of Y, denoted by F(y), is such that\n\n\nF(y) = P[Y \\le y] \\text{ for } -\\infty &lt; y &lt; \\infty\n\nTheorem: Properties of a Distribution Function\n\nIf F(y) is a distribution function, then\n\nF(-\\infty)   \\equiv \\underset{y \\to -\\infty}{\\lim} F(y) = 0\nF(\\infty)    \\equiv \\underset{y \\to \\infty}{\\lim} F(y) = 1\nF(y) is a nondecreasing function of y.\n\nIf y_1 and y_2 are any values such that y_1 &lt; y_2, then F(y_1) \\le F(y_2).\n\n\n\nNote: To be rigorous, if F(y) is a valid distribution function, then F(y) also must be right continuous."
  },
  {
    "objectID": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv-1",
    "href": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nContinuous random variable:\n\nA random variable Y with distribution function F(y) is said to be continuous if F(y) is continuous for -\\infty &lt; y &lt; \\infty.\n\nNote: To be mathematically precise, we also need the first derivative of F(y) to exist and be continuous.\n\n\nIf Y is a continuous random variable, then for any real number y, P[Y = y] = 0.\n\ni.e., we must remember to find the probability of an interval."
  },
  {
    "objectID": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv-2",
    "href": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv-2",
    "title": "Random Variables and their Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nProbability density function:\n\nLet F(y) be the cumulative density function for a continuous random variable, Y. Then\n\n\np[Y = y] = f(y) = \\frac{dF(y)}{dy} = F'(y).\n\nTheorem: Properties of a Density Function\n\nIf f(y) is a density function for a continuous random variable, then\n\nf(y) \\ge 0 \\ \\forall y, \\ -\\infty &lt; y &lt; \\infty.\n\\int_{-\\infty}^{\\infty} f(y) dy = 1."
  },
  {
    "objectID": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv-3",
    "href": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv-3",
    "title": "Random Variables and their Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nCumulative density function:\n\nLet f(y) be the probability density function for a continuous random variable, Y. Then\n\n\nP[Y \\le y] = F(y) = \\int_{-\\infty}^y f(t) dt, \\text{ for } y \\in {\\rm I\\!R}."
  },
  {
    "objectID": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv-4",
    "href": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv-4",
    "title": "Random Variables and their Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nSuppose that \nF(x) =\n\\begin{cases}\n0, & y &lt; 0 \\\\\ny, & 0 \\leq y \\leq 1 \\\\\n1, & y &gt; 1\n\\end{cases}\n\nFind the probability density function for Y."
  },
  {
    "objectID": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv-5",
    "href": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv-5",
    "title": "Random Variables and their Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nTheorem\n\nIf the random variable Y has density function f(y) and a &lt; b, then the probability that Y falls in the interval [a, b] is\n\n\n\nP[a \\le Y \\le b] = \\int_a^b f(y) dy."
  },
  {
    "objectID": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv-6",
    "href": "files/slides/intro-rv.html#probability-distributions-for-continuous-rv-6",
    "title": "Random Variables and their Distributions",
    "section": "4.2: Probability Distributions for Continuous RV",
    "text": "4.2: Probability Distributions for Continuous RV\n\nGiven f(y) = cy^2, where 0 \\le y \\le 2 and f(y)=0 elsewhere,\n\nFind the value of c for which f(y) is a valid density function. \nFind P[1 \\le Y \\le 2]."
  },
  {
    "objectID": "files/slides/intro-rv.html#expected-values-for-continuous-rv",
    "href": "files/slides/intro-rv.html#expected-values-for-continuous-rv",
    "title": "Random Variables and their Distributions",
    "section": "4.3: Expected Values for Continuous RV",
    "text": "4.3: Expected Values for Continuous RV\n\nExpected value:\n\nThe expected value of a continuous variable Y is\n\n\n\nE[Y] = \\int_{-\\infty}^{\\infty} y f(y) \\ dy\n\n\nThis is the continuous version of the expected mean for a discrete random variable,\n\n\nE[Y] = \\sum_y y p(y)"
  },
  {
    "objectID": "files/slides/intro-rv.html#expected-values-for-continuous-rv-1",
    "href": "files/slides/intro-rv.html#expected-values-for-continuous-rv-1",
    "title": "Random Variables and their Distributions",
    "section": "4.3: Expected Values for Continuous RV",
    "text": "4.3: Expected Values for Continuous RV\n\nTheorem:\n\nLet g(Y) be a function of Y; then the expected value of g(Y) is given by\n\n\n\nE\\left[ g(Y) \\right] = \\int_{-\\infty}^{\\infty} g(y) f(y) \\ dy\n\n\nTheorem:\n\nLet c be a constant and let g(Y), g_1(Y), g_2(Y), ‚Ä¶, g_k(Y) be functions of a continuous random variable, Y. Then the following results hold:\n\nE[c] = c\nE\\left[cg(Y)] = cE[g(Y)\\right]\nE\\left[g_1(Y)+g_2(Y)+...+g_k(Y)\\right] = E\\left[ g_1(Y) \\right] + E\\left[ g_2(Y) \\right] + ... + E\\left[ g_k(Y) \\right]"
  },
  {
    "objectID": "files/slides/intro-rv.html#expected-values-for-continuous-rv-2",
    "href": "files/slides/intro-rv.html#expected-values-for-continuous-rv-2",
    "title": "Random Variables and their Distributions",
    "section": "4.3: Expected Values for Continuous RV",
    "text": "4.3: Expected Values for Continuous RV\n\nIn a previous example, we determined that f(y) = \\frac{3}{8}y^2 for 0 \\le y \\le 2 and f(y) = 0 elsewhere is a valid density function.\nIf the random variable Y has this density function, find \\mu = E[y] and \\sigma^2 = V[Y]"
  },
  {
    "objectID": "files/slides/intro-rv.html#uniform-probability-distribution",
    "href": "files/slides/intro-rv.html#uniform-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "4.4: Uniform Probability Distribution",
    "text": "4.4: Uniform Probability Distribution\n\nUniform Distribution"
  },
  {
    "objectID": "files/slides/intro-rv.html#uniform-probability-distribution-1",
    "href": "files/slides/intro-rv.html#uniform-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "4.4: Uniform Probability Distribution",
    "text": "4.4: Uniform Probability Distribution\n\nUniform Distribution\n\nIf \\theta_1 &lt; \\theta_2, a random variable Y is said to have a uniform distribution on the interval (\\theta_1, \\theta_2) iff\n\n\n\nf(y) = \\begin{cases}\n      \\frac{1}{\\theta_2 - \\theta_1}, & \\theta_1 \\le y \\le \\theta_2 \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nTheorem:\n\nIf \\theta_1 &lt; \\theta_2 and Y is a random variable uniformly distributed on the interval (\\theta_1, \\theta_2), then\n\n\n\nE[Y] = \\mu = \\frac{\\theta_1+\\theta_2}{2} \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2 = \\frac{(\\theta_2-\\theta_1)^2}{12}\n\n\nSee Wackerly pg. 176 for derivation."
  },
  {
    "objectID": "files/slides/intro-rv.html#uniform-probability-distribution-2",
    "href": "files/slides/intro-rv.html#uniform-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "4.4: Uniform Probability Distribution",
    "text": "4.4: Uniform Probability Distribution\n\nThe change in depth of a river from one day to the next, measured in feet, at a specific location is a random variable, Y, with the following density function:\n\n\nf(y) = \\begin{cases}\n      k, & -2 \\le y \\le 2 \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nDetermine the value of k. \nFind the distribution function for Y."
  },
  {
    "objectID": "files/slides/intro-rv.html#uniform-probability-distribution-3",
    "href": "files/slides/intro-rv.html#uniform-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "4.4: Uniform Probability Distribution",
    "text": "4.4: Uniform Probability Distribution\n\nThe change in depth of a river from one day to the next, measured in feet, at a specific location is a random variable, Y, with the following density function:\n\n\nf(y) = \\begin{cases}\n      k, & -2 \\le y \\le 2 \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nWhat is the mean of the distribution? \nWhat is the variance of the distribution?"
  },
  {
    "objectID": "files/slides/intro-rv.html#normal-probability-distribution",
    "href": "files/slides/intro-rv.html#normal-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "4.5: Normal Probability Distribution",
    "text": "4.5: Normal Probability Distribution\n\nNormal Distribution"
  },
  {
    "objectID": "files/slides/intro-rv.html#normal-probability-distribution-1",
    "href": "files/slides/intro-rv.html#normal-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "4.5: Normal Probability Distribution",
    "text": "4.5: Normal Probability Distribution\n\nNormal Distribution\n\nA random variable Y is said to have a normal distribution iff, for \\sigma &gt; 0 and -\\infty &lt; \\mu &lt; \\infty,\n\n\n\nf(y) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-(y-\\mu)^2/(2\\sigma^2)}\n\n\nTheorem:\n\nIf Y is a random variable normally distributed with parameters \\mu and \\sigma, then\n\n\n\nE[Y] = \\mu =  \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2"
  },
  {
    "objectID": "files/slides/intro-rv.html#normal-probability-distribution-2",
    "href": "files/slides/intro-rv.html#normal-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "4.5: Normal Probability Distribution",
    "text": "4.5: Normal Probability Distribution\n\nThe weekly amount of money spent on maintenance and repairs by a company was observed over a long period of time to be approximately normally distributed with mean $400 and standard deviation $20.\n\n\nIf $450 is budgeted for next week, what is the probability that the actual costs will exceed the budgeted amount? \nHow much should be budgeted for weekly repairs and maintenance to provide that the probability the budgeted amount will be exceeded in a given week is only 0.10?"
  },
  {
    "objectID": "files/slides/intro-rv.html#gamma-probability-distribution",
    "href": "files/slides/intro-rv.html#gamma-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "4.6: Gamma Probability Distribution",
    "text": "4.6: Gamma Probability Distribution\n\nGamma Distribution"
  },
  {
    "objectID": "files/slides/intro-rv.html#gamma-probability-distribution-1",
    "href": "files/slides/intro-rv.html#gamma-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "4.6: Gamma Probability Distribution",
    "text": "4.6: Gamma Probability Distribution\n\nGamma Distribution\n\nA random variable Y is said to have a gamma distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0 iff,\n\n\n\nf(y) = \\begin{cases}\n      \\frac{y^{\\alpha-1} e^{-y/\\beta}}{\\beta^{\\alpha} \\Gamma(\\alpha)}, & 0 \\le y &lt; \\infty \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nNote that \\Gamma(\\alpha) = \\int_{0}^{\\infty} y^{\\alpha-1} e^{-y} \\ dy.\nTheorem:\n\nIf Y has a gamma distribution with parameters \\alpha and \\beta, then\n\n\n\nE[Y] = \\mu = \\alpha\\beta \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2  = \\alpha\\beta^2\n\n\nSee Wackerly pg. 187 for derivation."
  },
  {
    "objectID": "files/slides/intro-rv.html#gamma-probability-distribution-2",
    "href": "files/slides/intro-rv.html#gamma-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "4.6: Gamma Probability Distribution",
    "text": "4.6: Gamma Probability Distribution\n\nAnnual incomes for heads of household in a section of a city have approximately a gamma distribution with \\alpha=20 and \\beta=1000.\n\nWhat is f(y)?\nWhat is the mean of Y?\nWhat is the variance of Y?\nWhat proportion of heads of households in this section of the city have incomes in excess of $30,000?"
  },
  {
    "objectID": "files/slides/intro-rv.html#beta-probability-distribution",
    "href": "files/slides/intro-rv.html#beta-probability-distribution",
    "title": "Random Variables and their Distributions",
    "section": "4.7: Beta Probability Distribution",
    "text": "4.7: Beta Probability Distribution\n\nBeta Distribution"
  },
  {
    "objectID": "files/slides/intro-rv.html#beta-probability-distribution-1",
    "href": "files/slides/intro-rv.html#beta-probability-distribution-1",
    "title": "Random Variables and their Distributions",
    "section": "4.7: Beta Probability Distribution",
    "text": "4.7: Beta Probability Distribution\n\nBeta Distribution\n\nA random variable Y is said to have a beta distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0 iff,\n\n\n\nf(y) = \\begin{cases}\n      \\frac{y^{\\alpha-1}(1-y)^{\\beta-1}}{B(\\alpha,\\beta)}, & 0 \\le y \\le 1 \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nNote that B(\\alpha,\\beta) = \\int_0^1 y^{\\alpha-1}(1-y)^{\\beta-1} \\ dy = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}.\nTheorem:\n\nIf Y has a beta distribution with parameters \\alpha &gt; 0 and \\beta &gt; 0, then\n\n\n\nE[Y] = \\mu = \\frac{\\alpha}{\\alpha+\\beta} \\ \\ \\ \\text{and} \\ \\ \\ V[Y] = \\sigma^2  = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}"
  },
  {
    "objectID": "files/slides/intro-rv.html#beta-probability-distribution-2",
    "href": "files/slides/intro-rv.html#beta-probability-distribution-2",
    "title": "Random Variables and their Distributions",
    "section": "4.7: Beta Probability Distribution",
    "text": "4.7: Beta Probability Distribution\n\nThe percentage of impurities per batch in a chemical product is a random variable Y with density function\n\n\nf(y) = \\begin{cases}\n      12y^2(1-y), & 0 \\le y \\le 1 \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nA batch with more than 40% impurities cannot be sold.\n\n\nUse integration to determine the probability that a randomly selected batch cannot be sold because of excessive impurities. \nUse R to find the answer to part (a)."
  },
  {
    "objectID": "files/slides/intro-rv.html#beta-probability-distribution-3",
    "href": "files/slides/intro-rv.html#beta-probability-distribution-3",
    "title": "Random Variables and their Distributions",
    "section": "4.7: Beta Probability Distribution",
    "text": "4.7: Beta Probability Distribution\n\nThe percentage of impurities per batch in a chemical product is a random variable Y with density function\n\n\nf(y) = \\begin{cases}\n      12y^2(1-y), & 0 \\le y \\le 1 \\\\\n      0, & \\text{elsewhere}\n\\end{cases}\n\n\nA batch with more than 40% impurities cannot be sold.\n\n\nFind the mean of the percentage of impurities in a randomly selected batch of the chemical. \nFind the variance of the percentage of impurities in a randomly selected batch of the chemical."
  },
  {
    "objectID": "files/slides/intro-rv.html#homework-1",
    "href": "files/slides/intro-rv.html#homework-1",
    "title": "Random Variables and their Distributions",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\n\n4.11\n4.14\n4.17\n4.21\n4.28\n4.45\n4.46\n4.48\n4.68\n\n\n\n¬†\n\n\n\n4.69\n4.70\n4.97\n4.98\n4.102\n4.124\n4.125\n4.128\n4.131"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Abbreviated Syllabus",
    "section": "",
    "text": "Note: this is an abbreviated syllabus. The full syllabus is available through Classmate and/or Canvas.\n\nInstructor Information\n\nDr.¬†Samantha Seals\nOffice: 4/344\n\n\n\nMeeting Times and Location\n\nTR 2:00 pm‚Äì4:40 pm\n\n2:00 pm‚Äì3:15 pm: Lecture\n3:30 pm‚Äì4:30 pm: Daily practice\n\nPhysical classroom: 4/406\nZoom classroom: see Canvas or full syllabus for link\n\n\n\nOffice Hours\n\nMonday: 2:00‚Äì4:00 pm (Central)\nWednesday: 2:00‚Äì4:00 pm (Central)\nOther times by appointment\n\n\n\nGrading and Evaluation\nThe course grade will be determined as follows:\n\nAssignments (25%): Every module will finish with an assignment to demonstrate the knowledge gained. These will be like the daily practice, but putting together. The resulting .html file should be submitted to the designated dropbox on Canvas by 11:59 pm on the specified date (see Canvas).\nProjects (50%): There are two group projects in this course. The first project will result in a recorded presentation while the second project will result in a live presentation. Students are required to attend class during live presentations on July 31, 2025.\nFinal Exam (25%): The final exam will be a timed, concepts-based exam. While there may be some calculations needed, you will not be processing raw data. The final exam will be on Canvas on August 5, 2025. All students will have 3 hours to complete the exam.\n\n\n\nLate Policy\nAssigments have due dates, however, the dropboxes will not close until the end of the semester. All students are automatically granted ‚Äúextensions‚Äù without question. Note that if there is not a submission when I go to grade (after the initial deadline), I will assign a zero (0) and request that you submit the assignment when you are able to. This is only for record keeping purposes. There is no penalty for submitting late and a full grade will be given upon review of your submission.\nExtensions are not available for projects or the final exam."
  }
]